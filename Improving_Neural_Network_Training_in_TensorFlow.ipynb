{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Improving_Neural_Network_Training_in_TensorFlow.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOPW4lROVYdhn2z6qSeB5p2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lukmanr/codenext/blob/master/Improving_Neural_Network_Training_in_TensorFlow.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ta2WuhvgIjRa",
        "colab_type": "text"
      },
      "source": [
        "# Improving Neural Network Training in TensorFlow\n",
        "\n",
        "This notebook shows you how to improve training for the basic Neural Network we saw in the previous notebook. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tLQWJal3_Hv0",
        "colab_type": "text"
      },
      "source": [
        "![two layer MNIST network](https://i.imgur.com/5LOj7mtl.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0jzreYS1JH4E",
        "colab_type": "text"
      },
      "source": [
        "Let's start with the code from the basic Neural Network in TensorFlow we did last time.  We are going to \"refactor\", or reorganize, the code a little.  We'll put all the code that loads the MNIST training data into one method, ```load_training_data```."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uxLvKF-kYASA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%tensorflow_version 2.x\n",
        "import tensorflow as tf\n",
        "\n",
        "# weights W[784, 10]   784=28*28\n",
        "W = tf.Variable(tf.zeros([784, 10]))\n",
        "\n",
        "# biases b[10]\n",
        "b = tf.Variable(tf.zeros([10]))\n",
        "\n",
        "# Load the MNIST data set. The training set and test set are split\n",
        "# automatically. We pre-process the data to \"normalize\" the pixels (make all \n",
        "# the pixel values between 0 and 1) and reshape the data into 2-D tensors of\n",
        "# shape 1 x 784.\n",
        "def load_training_data():\n",
        "  (x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
        "\n",
        "  # convert the integer pixel values to floats\n",
        "  x_train = x_train / 255.0\n",
        "  x_test = x_test / 255.0\n",
        "\n",
        "  # reshape the images to be 2-D tensors of 1 x 784 pixels\n",
        "  x_train = x_train.reshape([-1, 1, 784])\n",
        "  x_test = x_test.reshape([-1, 1, 784])\n",
        "\n",
        "  return (x_train, y_train), (x_test, y_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1PAIkeTAJKoi",
        "colab_type": "text"
      },
      "source": [
        "The code that defines the neural network will be the same, for now.  Remember from last time:  the network is a simple function that takes the inputs X as argument.  It computes the input to each neuron by multiplying the inputs times the weights and adding the biases.  Then it applies the \"softmax\" activation function to the inputs, to compute the output for each of the 10 neurons."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yh-hcgQtYJ3U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# The neural network\n",
        "def neural_network(X):\n",
        "  Inputs = tf.matmul(X, W) + b\n",
        "  Y = tf.nn.softmax(Inputs)\n",
        "  return Y"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_RYUsshBLGNQ",
        "colab_type": "text"
      },
      "source": [
        "Now we define the neural network itself.  The network is a simple function that takes the inputs X as argument.  It computes the input to each neuron by multiplying the inputs times the weights and adding the biases.  Then it applies the \"softmax\" activation function to the inputs, to compute the output for each of the 10 neurons.  The softmax function enforces the sum of the outputs to be equal to 1, and it makes the high outputs higher and the low outputs lower, which helps the network \"make a choice\" between the 10 different digits. The method returns the outputs, a Tensor of 10 elements."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4FKQ3o13PuCy",
        "colab_type": "text"
      },
      "source": [
        "![two layer MNIST network](https://i.imgur.com/5LOj7mtl.png)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A8iMYM39gfIw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# The neural network\n",
        "def neural_network(X):\n",
        "  Inputs = tf.matmul(X, W) + b\n",
        "  Y = tf.nn.softmax(Inputs)\n",
        "  return Y"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oGxF-8xZMkGj",
        "colab_type": "text"
      },
      "source": [
        "The loss function is the same sum of squares function we have seen before."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QsmqazsridHB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# The loss function\n",
        "def loss(Y, Y_l):\n",
        "  return tf.reduce_sum(tf.square(Y - Y_l))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4k5lJp1-MuEA",
        "colab_type": "text"
      },
      "source": [
        "Here is the training loop."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oT7AiAqHlHWl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "num_epochs = 1\n",
        "learning_rate = 0.001\n",
        "\n",
        "counter = 0\n",
        "eval_steps = 500\n",
        "total_loss = 0.0\n",
        "\n",
        "# the outer training loop:  repeat for num_epochs\n",
        "for e in range(num_epochs):\n",
        "\n",
        "    # the inner training loop: train on one image and label from the data set\n",
        "    for image, label in zip(x_train, y_train):\n",
        "\n",
        "        # use this to count how many training loops we've executed\n",
        "        counter += 1\n",
        "\n",
        "        # convert the image and label to tensors\n",
        "        X = tf.Variable(image, dtype=tf.float32)\n",
        "        Y_l = tf.Variable(label, dtype=tf.float32)        \n",
        "\n",
        "        # we wrap this 'with' statement around the next two lines, to tell \n",
        "        # TensorFlow to auto-compute the gradients\n",
        "        with tf.GradientTape() as tape:\n",
        "            # now get the output of the neural net\n",
        "            Y = neural_network(X)\n",
        "\n",
        "            # compute the loss function \n",
        "            current_loss = loss(Y, Y_l)\n",
        "\n",
        "        # compute the gradients of the weights and biases with respect to the\n",
        "        # loss function\n",
        "        dW, db = tape.gradient(current_loss, [W, b])\n",
        "\n",
        "        # update the weights and biases. Remember we need to reverse the sign; \n",
        "        # ie. we want the add the negative of the gradient (times the \n",
        "        # learning rate) to the weights and biases.\n",
        "        W.assign(W - learning_rate * dW)\n",
        "        b.assign(b - learning_rate * db)\n",
        "\n",
        "        # every eval_steps, print the average loss since the last eval\n",
        "        total_loss += current_loss.numpy()\n",
        "        if (counter % eval_steps == 0):\n",
        "            avg_loss = total_loss / eval_steps\n",
        "            print(avg_loss)\n",
        "            total_loss = 0.0"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}