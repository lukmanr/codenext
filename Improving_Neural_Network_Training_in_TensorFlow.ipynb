{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Improving_Neural_Network_Training_in_TensorFlow.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMNrczkslpuvKH6OjoreWK3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lukmanr/codenext/blob/master/Improving_Neural_Network_Training_in_TensorFlow.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ta2WuhvgIjRa",
        "colab_type": "text"
      },
      "source": [
        "# Improving Neural Network Training in TensorFlow\n",
        "\n",
        "This notebook shows you how to improve training for the basic Neural Network we saw in the previous notebook. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tLQWJal3_Hv0",
        "colab_type": "text"
      },
      "source": [
        "![two layer MNIST network](https://i.imgur.com/5LOj7mtl.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0jzreYS1JH4E",
        "colab_type": "text"
      },
      "source": [
        "Let's start with the code from the basic Neural Network in TensorFlow we did last time.  We are going to \"refactor\", or reorganize, the code a little.  We'll put all the code that loads the MNIST training data into one method, ```load_training_data```."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uxLvKF-kYASA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%tensorflow_version 2.x\n",
        "import tensorflow as tf\n",
        "\n",
        "# Load the MNIST data set. The training set and test set are split\n",
        "# automatically. We pre-process the data to \"normalize\" the pixels (make all \n",
        "# the pixel values between 0 and 1) and reshape the data into 2-D tensors of\n",
        "# shape 1 x 784.\n",
        "def load_training_data():\n",
        "  (x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
        "\n",
        "  # convert the integer pixel values to floats\n",
        "  x_train = x_train / 255.0\n",
        "  x_test = x_test / 255.0\n",
        "\n",
        "  # reshape the images to be 2-D tensors of 1 x 784 pixels\n",
        "  x_train = x_train.reshape([-1, 1, 784])\n",
        "  x_test = x_test.reshape([-1, 1, 784])\n",
        "\n",
        "  return (x_train, y_train), (x_test, y_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1PAIkeTAJKoi",
        "colab_type": "text"
      },
      "source": [
        "The code that defines the neural network will be the same, for now.  Remember from last time:  the network is a simple function that takes the inputs X as argument.  It computes the input to each neuron by multiplying the inputs times the weights and adding the biases.  Then it applies the \"softmax\" activation function to the inputs, to compute the output for each of the 10 neurons."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "63CkAiRATFN4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# weights W[784, 10]   784=28*28\n",
        "W = tf.Variable(tf.zeros([784, 10]))\n",
        "\n",
        "# biases b[10]\n",
        "b = tf.Variable(tf.zeros([10]))\n",
        "\n",
        "# The neural network\n",
        "def neural_network(X):\n",
        "  Inputs = tf.matmul(X, W) + b\n",
        "  Y = tf.nn.softmax(Inputs)\n",
        "  return Y"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oGxF-8xZMkGj",
        "colab_type": "text"
      },
      "source": [
        "The loss function is the same sum of squares function we have seen before."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QsmqazsridHB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# The loss function\n",
        "def loss(Y, Y_l):\n",
        "  return tf.reduce_sum(tf.square(Y - Y_l))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4k5lJp1-MuEA",
        "colab_type": "text"
      },
      "source": [
        "Here is the training loop.  We'll put that into a method as well, to make it easier to call repeatedly.  The method takes ```num_epochs```, ```learning_rate```, and ```eval_steps``` as hyperparameters, specifying the default values of each of those parameters.  We've also modified it to plot the average loss values every ```eval_steps``` if the plot flag is set to True. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oT7AiAqHlHWl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def nn_training_loop(num_epochs = 1, \n",
        "                     learning_rate = 0.001, \n",
        "                     eval_steps = 500,\n",
        "                     plot = False):\n",
        "\n",
        "  (x_train, y_train), (x_test, y_test) = load_training_data()\n",
        "\n",
        "  counter = 0\n",
        "  total_loss = 0.0\n",
        "  avg_losses = []\n",
        "\n",
        "  # the outer training loop:  repeat for num_epochs\n",
        "  for e in range(num_epochs):\n",
        "\n",
        "      # the inner training loop: train on one image and label from the data set\n",
        "      for image, label in zip(x_train, y_train):\n",
        "\n",
        "          # use this to count how many training loops we've executed\n",
        "          counter += 1\n",
        "\n",
        "          # convert the image and label to tensors\n",
        "          X = tf.Variable(image, dtype=tf.float32)\n",
        "          Y_l = tf.Variable(label, dtype=tf.float32)        \n",
        "\n",
        "          # we wrap this 'with' statement around the next two lines, to tell \n",
        "          # TensorFlow to auto-compute the gradients\n",
        "          with tf.GradientTape() as tape:\n",
        "              # get the output of the neural net\n",
        "              Y = neural_network(X)\n",
        "\n",
        "              # compute the loss function \n",
        "              current_loss = loss(Y, Y_l)\n",
        "\n",
        "          # compute the gradients of the weights and biases with respect to the\n",
        "          # loss function\n",
        "          dW, db = tape.gradient(current_loss, [W, b])\n",
        "\n",
        "          # update the weights and biases. Remember we need to reverse the sign; \n",
        "          # ie. we want the add the negative of the gradient (times the \n",
        "          # learning rate) to the weights and biases.\n",
        "          W.assign(W - learning_rate * dW)\n",
        "          b.assign(b - learning_rate * db)\n",
        "\n",
        "          # every eval_steps, print the average loss since the last eval\n",
        "          total_loss += current_loss.numpy()\n",
        "          if (counter % eval_steps == 0):\n",
        "              avg_loss = total_loss / eval_steps\n",
        "              avg_losses.append(avg_loss)\n",
        "              if plot:\n",
        "                plt.plot(avg_losses)\n",
        "                plt.show()\n",
        "              else:\n",
        "                print(\"avg_loss = \", avg_loss)\n",
        "              total_loss = 0.0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "62U-FpKEbZwH",
        "colab_type": "text"
      },
      "source": [
        "Now call the function to execute the loop."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FX1Qn8FqWqp5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "nn_training_loop()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sz9hQ--YbgUX",
        "colab_type": "text"
      },
      "source": [
        "  Note again that the loss does not seem to go down.  We need to make some improvements to this Neural Network in order for it to learn how to solve the MNIST data problem!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3OmXWdLlVuta",
        "colab_type": "text"
      },
      "source": [
        "## Improvements to training\n",
        "We are going to try these things to improve the training of this neural network:\n",
        "\n",
        "1.   Use **Stochastic Gradient Descent**:  train in batches.\n",
        "2.   Use **a better loss** function (the \"cross entropy loss\").\n",
        "3.   Add some **\"hidden layers\"**.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jqrsKP_yN1Q4",
        "colab_type": "text"
      },
      "source": [
        "First let's create a new method to create batches of data.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M_La_NOrWpTT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def batch_training_data(batch_size=100):\n",
        "  (x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
        "\n",
        "  # reshape the images to be 2-D tensors of 1 x 784 pixels\n",
        "  x_train = x_train.reshape([-1, 784])\n",
        "  x_train = tf.convert_to_tensor(x_train, dtype=tf.float32) / 255.\n",
        "  x_test = x_test.reshape([-1, 784])\n",
        "  x_test = tf.convert_to_tensor(x_test, dtype=tf.float32) / 255.\n",
        "\n",
        "  # convert the labels to floats\n",
        "  y_train = tf.convert_to_tensor(y_train, dtype=tf.float32)\n",
        "  y_test = tf.convert_to_tensor(y_test, dtype=tf.float32)\n",
        "\n",
        "  # create a TensorFlow dataset for train and test sets\n",
        "  train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
        "  test_dataset = tf.data.Dataset.from_tensor_slices((x_test, y_test))\n",
        "\n",
        "  # shuffle (randomly select) the dataset, and set the batch size\n",
        "  train_dataset = train_dataset.shuffle(10000).batch(batch_size)\n",
        "\n",
        "  return train_dataset, test_dataset"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pBRxy-6yb-Dh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "1372deeb-a70f-4f52-8e7e-addd3fb7e02a"
      },
      "source": [
        "batch_training_data()"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(<BatchDataset shapes: ((None, 784), (None,)), types: (tf.float32, tf.float32)>,\n",
              " <TensorSliceDataset shapes: ((784,), ()), types: (tf.float32, tf.float32)>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r1V0vDhtN-UL",
        "colab_type": "text"
      },
      "source": [
        "To calculate our final results we use this code."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xgYVqEMKVBY3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def test_results(test_data):\n",
        "\n",
        "  test_metric = tf.keras.metrics.SparseCategoricalAccuracy()\n",
        "  test_data = test_data.batch(1000)\n",
        "  for test_images, test_labels in test_data:\n",
        "    predictions = neural_network(test_images)\n",
        "    test_metric.update_state(test_labels, predictions)\n",
        "\n",
        "  print(\"Accuracy = \", test_metric.result().numpy() * 100)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z-6Ozn-QG-We",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def nn_training_loop_with_batches(num_epochs = 1, \n",
        "                                  learning_rate = 0.001, \n",
        "                                  eval_steps = 500, \n",
        "                                  batch_size = 100,\n",
        "                                  plot = False):\n",
        "\n",
        "  train_data, test_data = batch_training_data(batch_size)\n",
        "\n",
        "  counter = 0\n",
        "  total_loss = 0.0\n",
        "  avg_losses = []\n",
        "\n",
        "  # the outer training loop:  repeat for num_epochs\n",
        "  for e in range(num_epochs):\n",
        "\n",
        "      # the inner training loop: train on one image and label from the data set\n",
        "      for X, Y_l in train_data:\n",
        "\n",
        "          # use this to count how many training loops we've executed\n",
        "          counter += 1\n",
        "\n",
        "          # we wrap this 'with' statement around the next two lines, to tell \n",
        "          # TensorFlow to auto-compute the gradients\n",
        "          with tf.GradientTape() as tape:\n",
        "              # get the output of the neural net\n",
        "              Y = neural_network(X)\n",
        "\n",
        "              print(\"X.shape\", X.shape)\n",
        "\n",
        "              print(\"Y.shape\", Y.shape)\n",
        "\n",
        "              print(\"Y_l.shape\", Y_l.shape)\n",
        "\n",
        "              # compute the loss function \n",
        "              current_loss = loss(Y, Y_l)\n",
        "\n",
        "          break\n",
        "\n",
        "      break\n",
        "\n",
        "          # compute the gradients of the weights and biases with respect to the\n",
        "          # loss function\n",
        "          #dW, db = tape.gradient(current_loss, [W, b])\n",
        "\n",
        "          # update the weights and biases. Remember we need to reverse the sign; \n",
        "          # ie. we want the add the negative of the gradient (times the \n",
        "          # learning rate) to the weights and biases.\n",
        "          #W.assign(W - learning_rate * dW)\n",
        "          #b.assign(b - learning_rate * db)\n",
        "\n",
        "          # every eval_steps, print the average loss since the last eval\n",
        "          #total_loss += current_loss.numpy()\n",
        "          #if (counter % eval_steps == 0):\n",
        "          #    avg_loss = total_loss / eval_steps\n",
        "          #    avg_losses.append(avg_loss)\n",
        "          #    if (plot):\n",
        "          #      plt.plot(avg_losses)\n",
        "          #      plt.show()\n",
        "          #    else:\n",
        "          #      print(\"avg_loss = \", avg_loss)\n",
        "          #    print(\"counter = \", counter)\n",
        "          #    total_loss = 0.0\n",
        "\n",
        "          #test_results(test_data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sfuITEwBHus6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 273
        },
        "outputId": "9c5a2bf9-5cd3-4dfd-ade1-3aec7c5f2da0"
      },
      "source": [
        "nn_training_loop_with_batches()"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "X.shape (100, 784)\n",
            "Y.shape (100, 10)\n",
            "Y_l.shape (100,)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "InvalidArgumentError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-29-80bf5c42877c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnn_training_loop_with_batches\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-28-c67526fad06d>\u001b[0m in \u001b[0;36mnn_training_loop_with_batches\u001b[0;34m(num_epochs, learning_rate, eval_steps, batch_size, plot)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m               \u001b[0;31m# compute the loss function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m               \u001b[0mcurrent_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_l\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m           \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-3-eb60b72d2e55>\u001b[0m in \u001b[0;36mloss\u001b[0;34m(Y, Y_l)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# The loss function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_l\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce_sum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msquare\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mY_l\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py\u001b[0m in \u001b[0;36mbinary_op_wrapper\u001b[0;34m(x, y)\u001b[0m\n\u001b[1;32m    982\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    983\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 984\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    985\u001b[0m       \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msparse_tensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSparseTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    986\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_math_ops.py\u001b[0m in \u001b[0;36msub\u001b[0;34m(x, y, name)\u001b[0m\n\u001b[1;32m  10098\u001b[0m         \u001b[0;32mpass\u001b[0m  \u001b[0;31m# Add nodes to the TensorFlow graph.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m  10099\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m> 10100\u001b[0;31m       \u001b[0m_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m  10101\u001b[0m   \u001b[0;31m# Add nodes to the TensorFlow graph.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m  10102\u001b[0m   _, _, _op, _outputs = _op_def_library._apply_op_helper(\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mraise_from_not_ok_status\u001b[0;34m(e, name)\u001b[0m\n\u001b[1;32m   6651\u001b[0m   \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\" name: \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6652\u001b[0m   \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6653\u001b[0;31m   \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6654\u001b[0m   \u001b[0;31m# pylint: enable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6655\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/six.py\u001b[0m in \u001b[0;36mraise_from\u001b[0;34m(value, from_value)\u001b[0m\n",
            "\u001b[0;31mInvalidArgumentError\u001b[0m: Incompatible shapes: [100,10] vs. [100] [Op:Sub]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i_FUtCztZxIk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "_, test_data = batch_training_data()\n",
        "test_results(test_data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4RUNpjVBMSjI",
        "colab_type": "text"
      },
      "source": [
        "### A better loss function\n",
        "\n",
        "For a problem in which we are trying to classify a set of objects, a better loss function than \"squared error\" is \"cross entropy\".  It computes the difference between two probability distributions.  In this case, the output of the 10 neurons is a probablity distribution, and the label represents another probability distribution."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JkED7j_YI1nu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# The loss function\n",
        "def loss(Y, Y_l):\n",
        "  return tf.keras.losses.sparse_categorical_crossentropy(Y_l, Y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PmNL-9OLNejA",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dWUL9_3BNhdj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "nn_training_loop_with_batches()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EeO94bCjNiGy",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-7hRwXGyH2rk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# weights/biases for first layer\n",
        "W1 = tf.Variable(tf.zeros([784, 20]))\n",
        "b1 = tf.Variable(tf.zeros([20]))\n",
        "\n",
        "# weights/biases for second layer\n",
        "W2 = tf.Variable(tf.zeros([20, 20]))\n",
        "b2 = tf.Variable(tf.zeros([20]))\n",
        "\n",
        "# weights/biases for final layer\n",
        "W3 = tf.Variable(tf.zeros([20, 10]))\n",
        "b3 = tf.Variable(tf.zeros([10]))\n",
        "\n",
        "# The neural network\n",
        "def neural_network(X):\n",
        "  input1 = tf.matmul(X, W1) + b1\n",
        "  output1 = tf.nn.relu(input1)\n",
        "\n",
        "  input2 = tf.matmul(output1, W2) + b2\n",
        "  output2 = tf.nn.relu(input2)\n",
        "\n",
        "  input3 = tf.matmul(output2, W3) + b3\n",
        "  Y = tf.nn.softmax(input3)\n",
        "  return Y"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fDT_ziLreUi-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def multi_layer_nn_training_loop(num_epochs = 1, \n",
        "                                 learning_rate = 0.001, \n",
        "                                 eval_steps = 500, \n",
        "                                 batch_size = 100,\n",
        "                                 plot = False):\n",
        "\n",
        "  train_data, test_data = batch_training_data(batch_size)\n",
        "\n",
        "  counter = 0\n",
        "  total_loss = 0.0\n",
        "  avg_losses = []\n",
        "\n",
        "  # the outer training loop:  repeat for num_epochs\n",
        "  for e in range(num_epochs):\n",
        "\n",
        "      # the inner training loop: train on one image and label from the data set\n",
        "      for X, Y_l in train_data:\n",
        "\n",
        "          # use this to count how many training loops we've executed\n",
        "          counter += 1\n",
        "\n",
        "          # we wrap this 'with' statement around the next two lines, to tell \n",
        "          # TensorFlow to auto-compute the gradients\n",
        "          with tf.GradientTape() as tape:\n",
        "              # get the output of the neural net\n",
        "              Y = neural_network(X)\n",
        "\n",
        "              # compute the loss function \n",
        "              current_loss = loss(Y, Y_l)\n",
        "\n",
        "          # compute the gradients of the weights and biases with respect to the\n",
        "          # loss function\n",
        "          dW1, db1, dW2, db2, dW3, db3 = tape.gradient(current_loss, [W1, b1, W2, b2, W3, b3])\n",
        "\n",
        "          # update the weights and biases. Remember we need to reverse the sign; \n",
        "          # ie. we want the add the negative of the gradient (times the \n",
        "          # learning rate) to the weights and biases.\n",
        "          W1.assign(W1 - learning_rate * dW1)\n",
        "          b1.assign(b1 - learning_rate * db1)\n",
        "\n",
        "          W2.assign(W2 - learning_rate * dW2)\n",
        "          b2.assign(b2 - learning_rate * db2)\n",
        "\n",
        "          W3.assign(W3 - learning_rate * dW3)\n",
        "          b3.assign(b3 - learning_rate * db3)\n",
        "\n",
        "          # every eval_steps, print the average loss since the last eval\n",
        "          total_loss += current_loss.numpy()\n",
        "          if (counter % eval_steps == 0):\n",
        "              avg_loss = total_loss / eval_steps\n",
        "              avg_losses.append(avg_loss)\n",
        "              if (plot):\n",
        "                plt.plot(avg_losses)\n",
        "                plt.show()\n",
        "              else:\n",
        "                print(\"avg_loss = \", avg_loss)\n",
        "              print(\"counter = \", counter)\n",
        "              total_loss = 0.0\n",
        "\n",
        "  test_results(test_data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "leZOaQ4ee2lk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "multi_layer_nn_training_loop()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XeR8i6lDe6r3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}