{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Gradient_Descent_in_TensorFlow.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNZQdBEbxfdUv3/MIt7QNdz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lukmanr/codenext/blob/master/Gradient_Descent_in_TensorFlow.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L6Y_g2oUOXlc",
        "colab_type": "text"
      },
      "source": [
        "# Gradient Descent in TensorFlow\n",
        "This notebook will show you how to implement simple gradient descent in TensorFlow 2.0.\n",
        "\n",
        "First we define a simple loss function, that takes two input variables $w_1$ and $w_2$.  The loss function is:\n",
        "\n",
        "$$ L = w_1^2 - 3w_1 + w_2^2 $$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bmNgr5JnFRRZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%tensorflow_version 2.x\n",
        "import tensorflow as tf\n",
        "import random\n",
        "\n",
        "def loss(w1, w2): \n",
        "\treturn w1 ** 2.0 - w1 * 3  + w2 ** 2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0R0sWTubOwSe",
        "colab_type": "text"
      },
      "source": [
        "Let's make a 3D graph of this function to see what it looks like."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rDA8dSFzOwzP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "from matplotlib import cm\n",
        "from matplotlib.ticker import LinearLocator, FormatStrFormatter\n",
        "import numpy as np\n",
        "\n",
        "# create grid to plot\n",
        "x = np.arange(-10.0, 10.0, 0.1)\n",
        "y = np.arange(-10.0, 10.0, 0.1)\n",
        "W1, W2 = np.meshgrid(x, y)\n",
        "\n",
        "# function to plot\n",
        "L = loss(W1, W2)\n",
        "\n",
        "# configure plot colors and format\n",
        "fig = plt.figure()\n",
        "ax = fig.gca(projection='3d')\n",
        "surf = ax.plot_surface(W1, W2, L, rstride=1, cstride=1, \n",
        "                      cmap=cm.RdBu, linewidth=0, antialiased=False)\n",
        "ax.zaxis.set_major_locator(LinearLocator(10))\n",
        "ax.zaxis.set_major_formatter(FormatStrFormatter('%.01f'))\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZB-kvURhOrAU",
        "colab_type": "text"
      },
      "source": [
        "Before we look at the code to do gradient descent, let's define another function to initialize $w_1$ and $w_2$ to random values.  ```random.random()``` produces a random number between 0 and 1."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pZO-hIUtOsBs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def initialize_weights():\t\n",
        "\tw1 = tf.Variable((random.random()-0.5) * 20)\n",
        "\tw2 = tf.Variable((random.random()-0.5) * 20) \n",
        "\treturn w1, w2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OZucHGwUQcBH",
        "colab_type": "text"
      },
      "source": [
        "Let's see how to perform gradients using the TensorFlow function 'GradientTape()'.  When you enclose python code ```with tf.GradientTape() as tape```, TensorFlow automatically computes the gradient of the function enclosed in the ```with``` statement.  The gradient is then available as ```tape.gradient```."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OEdGGyt0RmfK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "w1, w2 = initialize_weights()\n",
        "\n",
        "with tf.GradientTape() as tape:\n",
        "\ty = loss(w1, w2)\n",
        "\n",
        "gradients = tape.gradient(y, [w1, w2])\n",
        "\n",
        "print(w1.numpy(), w2.numpy())\n",
        "\n",
        "print(gradients[0].numpy(), gradients[1].numpy())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ulMrEyx2TR6o",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "Here is the full loop to perform gradient descent.  In each step, we compute the gradients, multiply the gradients by a \"learning rate\", and subtract that amount from the weights.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HM0UhfyKGOwz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "w1, w2 = initialize_weights()\n",
        "\n",
        "learning_rate = 0.2\n",
        "\n",
        "for i in range(50):\t\n",
        "\twith tf.GradientTape() as tape:\n",
        "\t\ty = loss(w1, w2)\n",
        "\tgradients = tape.gradient(y, [w1, w2])\n",
        "\tprint ('y = {:.1f}, w1 = {:.1f}, w2 = {:.1f},  grads0 = {:.1f}, grads1 = {:.1f} '.format(y.numpy(), w1.numpy(), w2.numpy(), gradients[0].numpy(), gradients[1].numpy()))\n",
        "\n",
        "\tw1.assign(w1 - learning_rate * gradients[0].numpy())\n",
        "\tw2.assign(w2 - learning_rate * gradients[1].numpy())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mK0fLj2bTNst",
        "colab_type": "text"
      },
      "source": [
        "Your turn:  change the \"learning rate\" to 0.1 and rerun the gradient descent.  What changes, and why?"
      ]
    }
  ]
}