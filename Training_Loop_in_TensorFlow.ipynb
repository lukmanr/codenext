{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Training_Loop_in_TensorFlow.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNd1y+bPP0gcRa7QGvEHUWA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lukmanr/codenext/blob/master/Training_Loop_in_TensorFlow.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ta2WuhvgIjRa",
        "colab_type": "text"
      },
      "source": [
        "# Neural Network Training Loop in TensorFlow\n",
        "\n",
        "This notebook shows you how to create a basic Neural Network training loop in TensorFlow.  The MNIST dataset is used to train a simple two-layer neural network."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0jzreYS1JH4E",
        "colab_type": "text"
      },
      "source": [
        "First we import TensorFlow as usual:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uxLvKF-kYASA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%tensorflow_version 2.x\n",
        "import tensorflow as tf"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1PAIkeTAJKoi",
        "colab_type": "text"
      },
      "source": [
        "Next we define the weights and the biases.  The weights are a two dimensional array, connecting the 784 pixels of input to the 10 neurons in the output layer.  There is one bias per neuron."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yh-hcgQtYJ3U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# weights W[784, 10]   784=28*28\n",
        "W = tf.Variable(tf.zeros([784, 10]))\n",
        "\n",
        "# biases b[10]\n",
        "b = tf.Variable(tf.zeros([10]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TEaM1V4MKA9k",
        "colab_type": "text"
      },
      "source": [
        "We use TensorFlow's keras.datasets module to load MNIST.  ```x_train, y_train``` are numpy arrays contain the training images and the training labels, respectively.  ```x_test, y_test``` are numpy arrays containing the test images and the test labels."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "peKxURjDdjLc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# load the MNIST data set. The training set and test set are split\n",
        "# automatically. \n",
        "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gaPxNDPrKeiN",
        "colab_type": "text"
      },
      "source": [
        "Next we do a little preprocessing of the training and test data.  The images are composed of 1 byte per pixel, each byte being a number from 0 to 255.  We divide the arrays by 255 to make each pixel go from 0 to 1.  We also have to \"flatten\" the images, to change their shape from 28 x 28 to 1 x 784.  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aBJsiF63gSrE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# convert the integer pixel values to floats\n",
        "x_train = x_train / 255.0\n",
        "x_test = x_test / 255.0\n",
        "\n",
        "# reshape the images to be 2-D tensors of 1 x 784 pixels\n",
        "x_train = x_train.reshape([-1, 1, 784])\n",
        "x_test = x_test.reshape([-1, 1, 784])\n",
        "\n",
        "# reshape the labels to be one hot encoded tensors\n",
        "y_train = tf.one_hot(y_train, depth=10)\n",
        "y_test = tf.one_hot(y_test, depth=10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_RYUsshBLGNQ",
        "colab_type": "text"
      },
      "source": [
        "Now we define the neural network itself.  The network is a simple function that takes the inputs X as argument.  It computes the input to each neuron by multiplying the inputs times the weights and adding the biases.  Then it applies the \"softmax\" activation function to the inputs, to compute the output for each of the 10 neurons.  The softmax function enforces the sum of the outputs to be equal to 1, and it makes the high outputs higher and the low outputs lower, which helps the network \"make a choice\" between the 10 different digits. The method returns the outputs, a Tensor of 10 elements."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4FKQ3o13PuCy",
        "colab_type": "text"
      },
      "source": [
        "![two layer MNIST network](https://i.imgur.com/5LOj7mtl.png)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A8iMYM39gfIw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# The neural network\n",
        "def neural_network(X):\n",
        "  Inputs = tf.matmul(X, W) + b\n",
        "  Y = tf.nn.softmax(Inputs)\n",
        "  return Y"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oGxF-8xZMkGj",
        "colab_type": "text"
      },
      "source": [
        "The loss function is the same sum of squares function we have seen before."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QsmqazsridHB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# The loss function\n",
        "def loss(Y, Y_l):\n",
        "  return tf.reduce_sum(tf.square(Y - Y_l))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4k5lJp1-MuEA",
        "colab_type": "text"
      },
      "source": [
        "Here is the training loop."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oT7AiAqHlHWl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "num_epochs = 1\n",
        "learning_rate = 0.001\n",
        "\n",
        "counter = 0\n",
        "eval_steps = 500\n",
        "total_loss = 0.0\n",
        "\n",
        "# the outer training loop:  repeat for num_epochs\n",
        "for e in range(num_epochs):\n",
        "\n",
        "    # the inner training loop: train on one image and label from the data set\n",
        "    for image, label in zip(x_train, y_train):\n",
        "\n",
        "        # use this to count how many training loops we've executed\n",
        "        counter += 1\n",
        "\n",
        "        # convert the image and label to tensors\n",
        "        X = tf.Variable(image, dtype=tf.float32)\n",
        "        Y_l = tf.Variable(label, dtype=tf.float32)        \n",
        "\n",
        "        # we wrap this 'with' statement around the next two lines, to tell \n",
        "        # TensorFlow to auto-compute the gradients\n",
        "        with tf.GradientTape() as tape:\n",
        "            # now get the output of the neural net\n",
        "            Y = neural_network(X)\n",
        "\n",
        "            # compute the loss function \n",
        "            current_loss = loss(Y, Y_l)\n",
        "\n",
        "        # compute the gradients of the weights and biases with respect to the\n",
        "        # loss function\n",
        "        dW, db = tape.gradient(current_loss, [W, b])\n",
        "\n",
        "        # update the weights and biases. Remember we need to reverse the sign; \n",
        "        # ie. we want the add the negative of the gradient (times the \n",
        "        # learning rate) to the weights and biases.\n",
        "        W.assign(W - learning_rate * dW)\n",
        "        b.assign(b - learning_rate * db)\n",
        "\n",
        "        # every eval_steps, print the average loss since the last eval\n",
        "        total_loss += current_loss.numpy()\n",
        "        if (counter % eval_steps == 0):\n",
        "            avg_loss = total_loss / eval_steps\n",
        "            print(\"train step =\", counter, \" avg loss =\", avg_loss)\n",
        "            total_loss = 0.0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5LE4S90o7YfJ",
        "colab_type": "text"
      },
      "source": [
        "Now let's evaluate the trained network against the test set.  This is easy once the test set is the correct shape.  We can just compute the loss on the entire test set, and get the average loss per test set item by dividing by the number of images in the test set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y5sCXB_f7cEa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# First convert the test set images and labels into Tensors\n",
        "x_test = tf.Variable(x_test, dtype=tf.float32)\n",
        "y_test = tf.Variable(y_test, dtype=tf.float32)\n",
        "\n",
        "# The labels need to be the same shape as the neural network output:\n",
        "# a vector of [1, 10].  The \"-1\" means, whatever dimension is needed\n",
        "# to get to the right shape.  In this case there are 10000 test images\n",
        "# so that \"-1\" is going to become 10000.\n",
        "y_test = tf.reshape(y_test, [-1, 1, 10])\n",
        "\n",
        "# Now compute the loss on the entire test set.  First compute the\n",
        "# neural network output on the entire test set:\n",
        "Y_test = neural_network(x_test)\n",
        "\n",
        "# Then use that output, along with the test labels, to compute the\n",
        "# loss for the entire test set:\n",
        "test_loss = loss(Y_test, y_test)\n",
        "\n",
        "test_loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vrtd1JNxD3ke",
        "colab_type": "text"
      },
      "source": [
        "This loss is the total loss for the entire test set.  Now compute the average loss for each test image by dividing by the number of test images.  This is the average network error for each test image."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pVnmiAAM7kJl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# divide by y_test.shape[0] which is the number of test images\n",
        "average_test_loss = test_loss / y_test.shape[0]\n",
        "\n",
        "# output the numpy value of the tensor (i.e. just the normal python value)\n",
        "average_test_loss.numpy()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}