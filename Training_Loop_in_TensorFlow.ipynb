{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Training_Loop_in_TensorFlow.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOxbf+Pi8NfsByytCpB+DXs",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lukmanr/codenext/blob/master/Training_Loop_in_TensorFlow.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ta2WuhvgIjRa",
        "colab_type": "text"
      },
      "source": [
        "# Neural Network Training Loop in TensorFlow\n",
        "\n",
        "This notebook shows you how to create a basic Neural Network training loop in TensorFlow.  The MNIST dataset is used to train a simple two-layer neural network."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0jzreYS1JH4E",
        "colab_type": "text"
      },
      "source": [
        "First we import TensorFlow as usual:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uxLvKF-kYASA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%tensorflow_version 2.x\n",
        "import tensorflow as tf"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1PAIkeTAJKoi",
        "colab_type": "text"
      },
      "source": [
        "Next we define the weights and the biases.  The weights are a two dimensional array, connecting the 784 pixels of input to the 10 neurons in the output layer.  There is one bias per neuron."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yh-hcgQtYJ3U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# weights W[784, 10]   784=28*28\n",
        "W = tf.Variable(tf.zeros([784, 10]))\n",
        "\n",
        "# biases b[10]\n",
        "b = tf.Variable(tf.zeros([10]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TEaM1V4MKA9k",
        "colab_type": "text"
      },
      "source": [
        "We use TensorFlow's keras.datasets module to load MNIST.  ```x_train, y_train``` are numpy arrays contain the training images and the training labels, respectively.  ```x_test, y_test``` are numpy arrays containing the test images and the test labels."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "peKxURjDdjLc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# load the MNIST data set. The training set and test set are split\n",
        "# automatically. \n",
        "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gaPxNDPrKeiN",
        "colab_type": "text"
      },
      "source": [
        "Next we do a little preprocessing of the training and test data.  The images are composed of 1 byte per pixel, each byte being a number from 0 to 255.  We divide the arrays by 255 to make each pixel go from 0 to 1.  We also have to \"flatten\" the images, to change their shape from 28 x 28 to 1 x 784.  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aBJsiF63gSrE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# convert the integer pixel values to floats\n",
        "x_train = x_train / 255.0\n",
        "x_test = x_test / 255.0\n",
        "\n",
        "# reshape the images to be 2-D tensors of 1 x 784 pixels\n",
        "x_train = x_train.reshape([-1, 1, 784])\n",
        "x_test = x_test.reshape([-1, 1, 784])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_RYUsshBLGNQ",
        "colab_type": "text"
      },
      "source": [
        "Now we define the neural network itself.  The network is a simple function that takes the inputs X as argument.  It computes the input to each neuron by multiplying the inputs times the weights and adding the biases.  Then it applies the \"softmax\" activation function to the inputs, to compute the output for each of the 10 neurons.  The softmax function enforces the sum of the outputs to be equal to 1, and it makes the high outputs higher and the low outputs lower, which helps the network \"make a choice\" between the 10 different digits. The method returns the outputs, a Tensor of 10 elements."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4FKQ3o13PuCy",
        "colab_type": "text"
      },
      "source": [
        "![two layer MNIST network](https://drive.google.com/file/d/1q9h9e0jhSCLrfr4Z4HZVnQbhIZcdWhTL/view?usp=sharing)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A8iMYM39gfIw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# The neural network\n",
        "def neural_network(X):\n",
        "  Inputs = tf.matmul(X, W) + b\n",
        "  Y = tf.nn.softmax(Inputs)\n",
        "  return Y"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oGxF-8xZMkGj",
        "colab_type": "text"
      },
      "source": [
        "The loss function is the same sum of squares function we have seen before."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QsmqazsridHB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# The loss function\n",
        "def loss(Y, Y_l):\n",
        "  return tf.reduce_sum(tf.square(Y - Y_l))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4k5lJp1-MuEA",
        "colab_type": "text"
      },
      "source": [
        "Here is the training loop."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oT7AiAqHlHWl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "num_epochs = 1\n",
        "learning_rate = 0.01\n",
        "\n",
        "# the outer training loop:  repeat for num_epochs\n",
        "for e in range(num_epochs):\n",
        "\n",
        "    # the inner training loop: train on one image and label from the data set\n",
        "    for image, label in zip(x_train, y_train):\n",
        "\n",
        "        # convert the image and label to tensors\n",
        "        X = tf.Variable(image, dtype=tf.float32)\n",
        "        Y_l = tf.Variable(label, dtype=tf.float32)        \n",
        "\n",
        "        # we wrap this 'with' statement around the next two lines, to tell \n",
        "        # TensorFlow to auto-compute the gradients\n",
        "        with tf.GradientTape() as tape:\n",
        "            # now get the output of the neural net\n",
        "            Y = neural_network(X, W, b)\n",
        "\n",
        "            # compute the loss function \n",
        "            current_loss = loss(Y, Y_l)\n",
        "\n",
        "        # compute the gradients of the weights and biases with respect to the\n",
        "        # loss function\n",
        "        dW, db = tape.gradient(current_loss, [W, b])\n",
        "\n",
        "        # update the weights and biases, by multiplying the gradients by the\n",
        "        # learning rate\n",
        "        W.assign(W + learning_rate * dW)\n",
        "        b.assign(b + learning_rate * db)\n",
        "\n",
        "        print(current_loss.numpy())"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}